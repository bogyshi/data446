\documentclass[11pt]{article}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb}
%\usepackage[dvips]{graphicx,color}
\usepackage{graphicx,color}
\usepackage{graphicx,color,bm}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{float}

%\setlength{\oddsidemargin}{0.1 in} \setlength{\evensidemargin}{-0.1
%in} \setlength{\topmargin}{-0.6 in} \setlength{\textwidth}{6.5 in}
%\setlength{\textheight}{8.5 in} \setlength{\headsep}{0.75 in}
%\setlength{\parindent}{0 in} \setlength{\parskip}{0.1 in}

\textwidth 6.3in \textheight 8.8in \topmargin -0.5truein
\oddsidemargin .15truein
\parskip .1in
\renewcommand{\baselinestretch}{1.53}  % double spaced


\newcommand{\homework}[9]{
	\pagestyle{myheadings}
	\thispagestyle{plain}
	\newpage
	\setcounter{page}{1}
	\noindent
	\begin{center}
		\framebox{
			\vbox{\vspace{2mm}
				\hbox to 6.28in { {\bf Math531:~Regression - I  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\Large \hfill #1 (#2)  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\it Instructor: #3 \hfill} }
				\hbox to 6.28in { {\it Office hours: #4  \hfill #6}}
				\vspace{2mm}}
		}
	\end{center}
	\markboth{#1}{#1}
	\vspace*{4mm}
}

% ----------------------- MATH -------------------------
\def\av{\boldsymbol a}
\def\bv{\boldsymbol b}
\def\cv{\boldsymbol c}
\def\dv{\boldsymbol d}
\def\ev{\boldsymbol e}
\def\fv{\boldsymbol f}
\def\gv{\boldsymbol g}
\def\hv{\boldsymbol h}
\def\iv{\boldsymbol i}
\def\gv{\boldsymbol j}
\def\kv{\boldsymbol k}
\def\lv{\boldsymbol l}
\def\mv{\boldsymbol m}
\def\nv{\boldsymbol n}
\def\ov{\boldsymbol o}
\def\pv{\boldsymbol p}
\def\qv{\boldsymbol q}
\def\rv{\boldsymbol r}
\def\sv{\boldsymbol s}
\def\tv{\boldsymbol t}
\def\uv{\boldsymbol u}
\def\vv{\boldsymbol v}
\def\wv{\boldsymbol w}
\def\xv{\boldsymbol x}
\def\yv{\boldsymbol y}
\def\zv{\boldsymbol z}
\def\Av{\boldsymbol A}
\def\Bv{\boldsymbol B}
\def\Cv{\boldsymbol C}
\def\Dv{\boldsymbol D}
\def\Ev{\boldsymbol E}
\def\Fv{\boldsymbol F}
\def\Gv{\boldsymbol G}
\def\Hv{\boldsymbol H}
\def\Iv{\boldsymbol I}
\def\Gv{\boldsymbol J}
\def\Kv{\boldsymbol K}
\def\Lv{\boldsymbol L}
\def\Mv{\boldsymbol M}
\def\Nv{\boldsymbol N}
\def\Ov{\boldsymbol O}
\def\Pv{\boldsymbol P}
\def\Qv{\boldsymbol Q}
\def\Rv{\boldsymbol R}
\def\Sv{\boldsymbol S}
\def\Tv{\boldsymbol T}
\def\Uv{\boldsymbol U}
\def\Vv{\boldsymbol V}
\def\Wv{\boldsymbol W}
\def\Xv{\boldsymbol X}
\def\Yv{\boldsymbol Y}
\def\Zv{\boldsymbol Z}
\def\Abf{\mathbf A}
\def\Bbf{\mathbf B}
\def\Cbf{\mathbf C}
\def\Dbf{\mathbf D}
\def\Ebf{\mathbf E}
\def\Fbf{\mathbf F}
\def\Gbf{\mathbf G}
\def\Hbf{\mathbf H}
\def\Ibf{\mathbf I}
\def\Gbf{\mathbf J}
\def\Kbf{\mathbf K}
\def\Lbf{\mathbf L}
\def\Mbf{\mathbf M}
\def\Nbf{\mathbf N}
\def\Obf{\mathbf O}
\def\Pbf{\mathbf P}
\def\Qbf{\mathbf Q}
\def\Rbf{\mathbf R}
\def\Sbf{\mathbf S}
\def\Tbf{\mathbf T}
\def\Ubf{\mathbf U}
\def\Vbf{\mathbf V}
\def\Wbf{\mathbf W}
\def\Xbf{\mathbf X}
\def\Ybf{\mathbf Y}
\def\Jbf{\mathbf J}
\def\Zbf{\mathbf Z}
\def\Am{\mathrm A}
\def\Bm{\mathrm B}
\def\Cm{\mathrm C}
\def\Dm{\mathrm D}
\def\Em{\mathrm E}
\def\Fm{\mathrm F}
\def\Gm{\mathrm G}
\def\Hm{\mathrm H}
\def\Im{\mathrm I}
\def\Gm{\mathrm J}
\def\Km{\mathrm K}
\def\Lm{\mathrm L}
\def\Mm{\mathrm M}
\def\Nm{\mathrm N}
\def\Om{\mathrm O}
\def\Pm{\mathrm P}
\def\Qm{\mathrm Q}
\def\Rm{\mathrm R}
\def\Sm{\mathrm S}
\def\Tm{\mathrm T}
\def\Um{\mathrm U}
\def\mv{\mathrm V}
\def\Wm{\mathrm W}
\def\Xm{\mathrm X}
\def\Ym{\mathrm Y}
\def\Zm{\mathrm Z}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}
\newcommand{\alphav}{\mbox{\boldmath{$\alpha$}}}
\newcommand{\betav}{\mbox{\boldmath{$\beta$}}}
\newcommand{\gammav}{\mbox{\boldmath{$\gamma$}}}
\newcommand{\deltav}{\mbox{\boldmath{$\delta$}}}
\newcommand{\epsilonv}{\mbox{\boldmath{$\epsilon$}}}
\newcommand{\zetav}{\mbox{\boldmath$\zeta$}}
\newcommand{\etav}{\mbox{\boldmath{$\eta$}}}
\newcommand{\iotav}{\mbox{\boldmath{$\iota$}}}
\newcommand{\kappav}{\mbox{\boldmath{$\kappa$}}}
\newcommand{\lambdav}{\mbox{\boldmath{$\lambda$}}}
\newcommand{\muv}{\mbox{\boldmath{$\mu$}}}
\newcommand{\nuv}{\mbox{\boldmath{$\nu$}}}
\newcommand{\xiv}{\mbox{\boldmath{$\xi$}}}
\newcommand{\omicronv}{\mbox{\boldmath{$\omicron$}}}
\newcommand{\piv}{\mbox{\boldmath{$\pi$}}}
\newcommand{\rhov}{\mbox{\boldmath{$\rho$}}}
\newcommand{\sigmav}{\mbox{\boldmath{$\sigma$}}}
\newcommand{\tauv}{\mbox{\boldmath{$\tau$}}}
\newcommand{\upsilonv}{\mbox{\boldmath{$\upsilon$}}}
\newcommand{\phiv}{\mbox{\boldmath{$\phi$}}}
\newcommand{\varphiv}{\mbox{\boldmath{$\varphi$}}}
\newcommand{\chiv}{\mbox{\boldmath{$\chi$}}}
\newcommand{\psiv}{\mbox{\boldmath{$\psi$}}}
\newcommand{\omegav}{\mbox{\boldmath{$\omega$}}}
\newcommand{\Sigmav}{\mbox{\boldmath{$\Sigma$}}}
\newcommand{\Lambdav}{\mbox{\boldmath{$\Lambda$}}}
\newcommand{\Deltav}{\mbox{\boldmath{$\Delta$}}}
\newcommand{\Omegav}{\mbox{\boldmath{$\Omega$}}}
\newcommand{\varepsilonv}{\mbox{\boldmath{$\varepsilon$}}}

\newcommand{\eps}{\varepsilon}
\newcommand{\epsv}{\mbox{\boldmath{$\varepsilon$}}}

\def\1v{\mathbf 1}
\def\0v{\mathbf 0}
\def\Id{\mathbf I} % identity matrix
\newcommand{\ind}[1]{\mathbbm{1}_{\left[ {#1} \right] }}
\newcommand{\Ind}[1]{\mathbbm{1}_{\left\{ {#1} \right\} }}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\QED}{\begin{flushright} {\bf QED} \end{flushright}}
\newcommand{\R}{\mathbb R}
\newcommand{\Real}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\E}{\mathbb E}
\newcommand{\sgn}{\mathop{\mathrm{sign}}}
\def\Pr{\mathrm P}
\def\pr{\mathrm P}
\newcommand{\Var}{\mathop{\rm Var}}
\newcommand{\var}{\mathop{\rm Var}}
\newcommand{\Cov}{\mathop{\rm Cov}}
\newcommand{\cov}{\mathop{\rm Cov}}
\newcommand{\Corr}{\mathop{\rm Corr}}
\newcommand{\ang}{\mathop{\rm Angle}}
\newcommand{\tr}{\mathop{\rm trace}}
\newcommand{\proj}{\mathop{\rm Proj}}
\newcommand{\rank}{\mathop{\rm rank}}

\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\Diag}{\mathop{\rm diag}}
\newcommand{\sk}{\vspace{0.5cm}}
\newcommand{\ds}{\displaystyle}
\newcommand{\mb}{\mbox}
\newcommand{\wh}{\widehat}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\To}{\longrightarrow}

\def\equalLaw{\stackrel{\mathcal{L}}{=}}
\def\equallaw{\stackrel{\mathcal{L}}{=}}

\def\half{\frac{1}{2}}

\usepackage{caption}

\begin{document}

\begin{title}
	{\Large\bf Final Review}
\end{title}

\author{\bf Alexander Van Roijen}

\maketitle

\newpage
Please complete the following:
\begin{enumerate}
\item Lecture 1. Basics of Probability
\begin{enumerate}
	\item \textbf{Definition (Sample space and event).} The sample space S of an experiment is the set of all possible outcomes
	of the experiment. An event A is a subset of the sample space S , and we say that A occurred if the
	actual outcome is in A.
	\item \textbf{Definition (General definition of probability).} A probability space consists of a sample space S and a
	probability function P(.) which takes an event A $\subset$ S as input and returns P(A), a real number between 0
	and 1, as output. The probability function must satisfy the following axioms: $P(\emptyset) = 0 \, P(S)=1$ and for a union of disjoint events, we get $P(A_1\cup...A_n) = P(A_1)+...P(A_n)$ 
	\item \textbf{Theorem. Properties of probability.} A probability function has the following properties, for any events A
	and B.
	\begin{enumerate}
		\item $P(A^c)=1-P(A)$
		\item if $A\subset B P(A)\le P(B)$
		\item $P(A\cup B) = P(A)+P(B) - P(A\cap B)$ \text{ which can be extended}\\
		$P(A\cup B \cup C) = P(A) + P(B) + P(C) - P(A\cap B) - P(A \cap C) - P(B \cap C) - P(A \cap B \cap C)$
	\end{enumerate}
	\item\textbf{ Definition (Conditional probability).} If A and B are events with $P(B) > 0$, then the conditional probability
	of A given B, denoted by $P(A|B)$, is defined as:
	$P(A | B) =\frac{
	P(A \cap B)}
	{P(B)}$
	Further note that all probabilities are in fact conditional. We like to think of P(A) as our prior beliefs of an event, and P(A|B) as our posterior, or what we think it is given something is already known.
	\item \textbf{Theorem.} For any events A1,..An with positive probabilities,\\
	$P(A1,...An) = P(A1)P(A2|A1)P(A3|A1,A2)...P(An|A1,...A_{n-1})$
	\item\textbf{ Theorem (Bayes’ rule).}
	$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
	\item \textcolor{red}{\textbf{Theorem (Law of total probability (LOTP)).}} Let A1,...An be a partition of the sample space S with
	$P(Ai) > 0$ for all i. Then\\
	$P(B) = \sum_{i=1}^{n}P(B|A_i)P(A_i)$\\
	When we condition on an event E, we update our beliefs
	to be consistent with this knowledge, effectively putting ourselves in a universe where we know that E
	occurred. Within our new universe, however, the laws of probability operate just as before. Conditional
	probability satisfies all the properties of probability!
	\item\textbf{ Theorem (Bayes’ rule with extra conditioning). }Provided that $P(A\cap E) > 0$ and $P(B \cap E) > 0$, we have
	$P(A | B, E) =
	\frac{P(B | A, E)P(A | E)}
	{P(B | E)}$
	\item T\textbf{heorem (Law of total probability (LOTP) with extra conditioning).} Let A1... An be a partition of the
	sample space S with $P(Ai | E) > 0$ for all i. Then \\
	$P(B|E) = \sum_{i=1}^{n}P(B|A_i,E)P(A_i|E)$
	\item\textbf{ Definition (Independence of two events).} Events A and B are independent if $P(A \cap B) = P(A)P(B)$. If
	$P(A) > 0$ and $P(B) > 0$, then this is equivalent with $P(A | B) = P(A)$, and also equivalent with $P(B | A) =
	P(B)$. Independence is a symmetric relation.
	\item \textbf{Proposition.} If A and B are independent, then $A^c$ and $B$ are independent, $A^c$ and $B^c$ are independent, and
	A and $B^c$ are independent.
	\item Definition (Independence of three events). Events A, B and C are said to be independent if all of the
	following relations hold:\\
	$P(A \cap B) = P(A)P(B);\\
	P(A \cap C) = P(A)P(C);\\
	P(B \cap C) = P(B)P(C);\\
	P(A \cap B \cap C) = P(A)P(B)P(C)$
	\item \textbf{Definition (Conditional independence).} Events A and B are said to be conditionally independent given E if
	$P(A \cap B | E) = P(A | E)P(B | E).$
	\item Problems shown: Monty Hall, and Positive test of conditionitis and bayes rule
\end{enumerate}
\item
\begin{enumerate}
	\item \textbf{Definition (Random variable).} Given an experiment with sample space S , a random variable is a function
	from the sample space S to the real numbers R. It is common, but not required, to denote random variables
	by capital letters. P(X=x) = P(X=X(s))
	\item Discrete PMFs are non negative, and sum to one over their support.
	\item \textbf{Definition (Bernoulli distribution). }An random variable X is said to have a Bernoulli distribution with
	parameter p if P(X = 1) = p and P(X = 0) = 1 $\sim$ p, where $0 < p < 1$. We write this as X $\sim$ Bern(p).
	\item \textbf{Theorem. }Let X $\sim Bin(n, p)$, and $q = 1 - p$ (often taken to denote the failure of a Bernoulli trial). Then
	$n - X \sim Bin(n, q)$.
	\item \textbf{Theorem (Hypergeometric PMF).} Consider an urn with w white balls and b blacks balls. We draw n balls
	out of the urn at random without replacement such that all the
	$\binom{w+b}{n}$
	samples are equally likely. Let X be the
	number of white balls in the sample. Then X is said to have the Hypergeometric distribution with parameters
	w, b and n: $X \sim HGeom(w, b, n)$. Then the PMF of X is\\
	$P(X=k)=\frac{\binom{w}{k}\binom{b}{n-k}}{\binom{b+w}{n}}$
	\item \textcolor{red}{\textbf{Theorem.}} If $X \sim HGeom(w, b, n)$ and $Y \sim HGeom(n,w+b-n,w)$, then X and Y have the same distribution.
	\item \textbf{Theorem.} If X $\sim$ Bin(n,p), Y $\sim$ Bin(m,p), and X is independent of Y, then the conditional distribution of
	$X|X+Y=r \sim$ hgeom(n,m,r).
	\item \textbf{Theorem (Binomial as a limiting case of the Hypergeometric).} If X$\sim$ HGeom(w, b, n) and N = w+b approaches infinity
	such that p = w/(w + b) remains fixed, then the PMF of X converges to the Bin(n, p) PMF.
	\item \textbf{ Theorem (PMF of g(X))}. Let X be a discrete random variable and $g R->R$. Then the support of g(X) is
	the set of all y such that g(x) = y for at least one x in the support of X, and the PMF of g(X) is
	$P(g(X) = Y) = \sum_{x:g(x)=y} P(X=x)$
	\item \textbf{Definition (Independence of two random variables).} Random variables X and Y are said to be independent
	if
	$P(X \le x, Y \le y) = P(X \le x)P(Y \le y)$
	for all $x, y \in R$. In the discrete case, this is equivalent to the condition
	P(X = x, Y = y) = P(X = x)P(Y = y);
	for all x in the support of X and all y in the support of Y.
\end{enumerate}
\item chapter 3: Expectation
\begin{enumerate}
	\item\textbf{ Proposition (Monotonicity of expectation).} Let X and Y be random variables such that X $\ge$ Y with probability 1. Then E(X)$\ge$E(Y), with equality holding if and only if X = Y with probability 1. 
	\item A hypergeometric can be considered a sum of bernoulli random variables, but with their probabilities conditioned on the previous iteration. However, when calculating hte expectation, each of the bernoullis are equally like to be picked first, so the expecation is n * p where p = w/w+b
	\item \textbf{Theorem. Properties of Indicator Random Variables}\\
	1. $I_A^k=I_A$\\
	2. $I_{A^c} = 1 - I_A$\\
	3. $I_{A\cap B} = I_AI_B$\\
	4. $I_{A\Cup B} - I_A + I_B - I_{A\cap B}$
	\item Inclusion Exclusion \\
	$P(A_1\cup ... A_n) = \sum_iP(A_i)-\sum_{j>i}P(A_i\cap A_j) + ... (-1)^nP(A_1...A_n)$
	\item Negative Binomial: Number of failures till a fixed number of successes. $=> NBin(r,p)P(X=x) = \binom{x+r-1}{r-1}p^rq^{x}$
	\item \textcolor{red}{\textbf{Poisson}} Note $\sum_{k=1}^{\infty}\frac{x^{k-1}}{(k-1)!} = e^x$ Useful for determining expectations and such. its a kernel
	\item \textbf{Theorem. Sum of independent poisson is poisson} if $X\sim Pois(\lambda_1) \, \, \, \& \, \, \, Y \sim Pois(\lambda_2) \, \, X+Y \sim Pois(\lambda_1+\lambda_2)$
	\item \textcolor{red}{\textbf{Theorem. Poisson given a sum of Poissons is Binomial}} $X\sim Pois(\lambda_1) \, \, \, \& \, \, \, Y \sim Pois(\lambda_2) \, \, \, PoisX=k|X+Y=n \sim Binom(n,\lambda_1/(\lambda_1+\lambda_2))$
\end{enumerate}
\item Chapter 4: Continuous
\begin{enumerate}
	\item \textbf{Universality of the Uniform}. This is useful as it can simulate a RV with any discrete Distribution from the Unif.
	\\
	ex\\
	$F(x) = \frac{e^x}{1+e^x} \, \, F^{-1}(u) = log(\frac{u}{1-u}) \sim $logistic with $U \sim Unif(0,1)$ 
	\item \textbf{Normal distribution}\\
	Note that the normal is symmetric, meaning tail areas are symmetric, andi n general $\phi(z)=\phi(-z)$
	\item \textbf{exponential distribution} This is the continuous version of the geometric distribution. $f(x) = \lambda e^{-\lambda x} x>0$
	\item \textcolor{red}{\textbf{Generalize exponential  }}$Y = \frac{X}{\lambda} \sim Expo(\lambda)$ with $X \sim Expo(1)$
	\item $E[X]$ with  $X\sim Expo(\lambda) = \frac{1}{\lambda}$
	\item $Var[X]$ with $ X\sim Expo(\lambda) = \frac{1}{\lambda^2}$
	\item Exponential RV is the only memoryless Random Variable $=> P(X\ge s + t| X\ge s ) = P(X\ge t)$
	\item \textcolor{red}{`\textbf{Poisson Process}} A poisson process is the process of arrivels with a rate $\lambda$ if \\
	1.The number of arrivals that occur in an interval of length t is a $Pois(\lambda t)$ random variable.\\
	2.The number of arrivals that occur in disjoint time intervals are independent of each other.
	\item doisjoint intervals of the poisson process are all exponential distributions, but their sums are not exponential, they are gamma
	\item see notes for further elaboration on this section and bayesian view point of statistics
\end{enumerate}
\item Chapter 5: Moments
\begin{enumerate}
	\item \textbf{ Definition (Median).} We say that c is a median of a random variable X if $P(X \le c) \ge .5 and P(X \ge c) \ge .5$
	\item \textbf{Definition (Mode).} For a discrete random variable X, we say that c is a mode of X if it maximizes the PMF:
	$P(X = c) \ge P(X = x)$ for all x. For a continuous random variable X with PDF f , we say that c is the mode
	if it maximizes the PDF: $f(c) \ge f (x)$ for all x.\\
	Note we can have multiple modes and medians but not means!
	\item \textbf{Theorem. }Let X be a random variable with mean $\mu$, and let m be the median of X.
	\\The value of c that minimizes the mean squared error $E(X - c)^2$ is c = $\mu$.
	\\A value of c that minimizes the mean absolute error $E|X - c|$ is c = m.
	\item\textbf{ Definition (Symmetry of a random variable). }We say that a random variable X has a symmetric distribution
	about $\mu$ if $X-\mu$ has the same distribution as $\mu - X$. The number $\mu$ must be the mean E(X) if it exists, and
	must also be a median of the distribution of X.
	\item \textcolor{red}{\textbf{Proposition (Symmetry in terms of the PDF).}} Let X be a continuous random variable with PDF f . Then X
	is symmetric about $\mu$ if and only if $f (x) = f (2\mu - x)$ for all x.
	\item\textbf{ Definition (Kinds of moments)}. Let X be a random variable with mean $\mu$ and variance $\sigma^2$. For any positive
	integer n, the nth moment of X is $E(X^n)$, the nth central moment is $E[(X-\mu)^n]$, and the nth standardized
	moment is$E((\frac{X-\mu}{\sigma^2})^n)$
	The mean is the first moment and the variance is the second central moment
	\item \textcolor{red}{\textbf{ Definition (Skewness).}} The skewness of a random variable X with mean $\mu$ and variance $\sigma^2$ is the third
	standardized moment of X \\
	Skew(X) = $E[(\frac{X-\mu}{\sigma^2})^3]$
	\item \textcolor{red}{\textbf{Proposition (Odd central moments of a symmetric distribution). }}Let X be symmetric about its mean $\mu$. Then
	for any odd number m, the mth central moment $E(X -\mu)^m$ is 0 if it exists.
	\item \textbf{kurtosis} This represents how diffent a distributiobns pdf is , large kurtosis means sharp peak in the middle, heavy tails, and low shoulders.
	\item \textbf{Definition (Sample moments)}. Let X1, X2, ... Xn be i.i.d. random variables. The kth sample moment is the random variable\\
	$M_k = \frac{1}{n}\sum_{j=1}^{n}X^k$\\ and
	$E[M_k] = E[\frac{1}{n}\sum_{j=1}^{n}X^k] = E[X_1^k]$ Hence it is an unbiased estimator of the true kth moment
	\item $Var(\bar{X_n}) = \frac{\sigma^2}{n}$
	\item \textbf{Sample variance} = $S_n^2=\frac{1}{n-1}\sum_{j=1}^{n}(X_j-\bar{X_n})^2$
	\item \textcolor{red}{\textbf{Example: Bernoulli MGF}}
	For $X \sim Bern(p)$, the MGF is
	$M(t) = E(e^{tX}) = e^{t0}P(X = 0) + e^{t}P(X = 1) = pet + q$
	where q = 1 - p. Since M(t) is finite for any t $\in$ R, the MGF of a Bernoulli random variable is defined on the entire real line.
	\item \textcolor{red}{\textbf{Example: Geometric MGF}}
	For $X \sim Geom(p)$, the MGF is
	$M(t) = E(e^{tX}) =
	\sum_{k=0}^{\infty}
	e^{tk}P(X = k) = \sum_{k=0}^{\infty}e^{tk}q^kp = \frac{p}{1-qe^t}$
	for $q^{et} < 1$ or, equivalently, for $(-\infty, log(1/q))$,
	\item \textbf{Theorem (Moments via derivatives of MGFs).} Given the MGF M(t) of a random variable X, we can get
	the nth moment of X by evaluating its nth derivative of the MGF at 0
	$E(X^n) = M^{(n')}(0)$
	\item \textcolor{red}{\textbf{Theorem (MGF determines the distribution). }}The MGF of a random variable determines its distribution: if
	two random variables have the same MGF, they must have the same distribution.
	\item \textcolor{red}{\textbf{Theorem (MGF of a sum of independent random variables).}} If X and Y are independent, then the MGF of
	X + Y is the product of the individual MGFs:
	$M_{X+Y} (t) = M_X(t)M_Y(t)$\\
	Using this result, we can obtain the MGFs of the Binomial and Negative Binomial, which are sums of
	independent Bernoulli and Geometric i.i.d. random variables.\\
	\textbf{Example: Binomial MGF}
	The MGF of a Bern(p) random variable is$ pe^t + q$, so the MGF of a Bin(n, p) random variable is
	$M(t) = (pe^t + q)^n$
\end{enumerate}
\item Chapter 6 : Joint Distributions
\begin{enumerate}
	\item \textcolor{red}{\textbf{Using 2d lotus to get Expected Value}}\\
	$X,Y \sim N(0,1) \text{ find }E[|X-Y|] = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty} |x-y|\frac{1}{\sqrt{2\pi}}e^{-x^2/2}\frac{1}{\sqrt{2\pi}}e^{-y^2/2}dxdy$\\
	we know $X-Y \sim N(0,2) => X-Y=\sqrt{2}Z $ with $ Z \sim N(0,1) =>
	E[|X-Y|] = \sqrt{2}E[|Z|] = \sqrt{2}\int_{-\infty}^{\infty}|z|e^{-z^2/2}dz$\\
	$=2\sqrt{2}\int_{0}^{\infty}ze^{-z^2/2}dz = \frac{2}{\sqrt{\pi}}$ \\
	\item \textcolor{red}{\textbf{Properties of Covariance}}// 
	4. Cov(aX,Y) = aCov(X,Y) for any constant a in R\\
	5. Cov(X + Y,Z) = Cov(X,Z) + Cov(Y,Z).\\
	6. Cov(X + Y,Z + W) = Cov(X,Z) + Cov(X,W) + Cov(Y,Z) + Cov(Y,W).\\
	7. Var(X + Y) = Var(X) + Var(Y) + 2Cov(X,Y).\\
	8. For any X1 X2 ... Xn  $Var(X1+X2+..Xn) = Var(X1)+...Var(Xn) + 2\sum_{i<j}Cov(X_i,X_j)$
	\item \textbf{Theorem (Multinomial joint PMF)}. If $X \sim Multk(n, p)$, then the joint PMF of the random vector X is
	$P(X_1 = n_1...X_k = n_k) = \frac{n!}{n_1!...n_k!}p_1^{n_1}...p_k^{n_k}$
	where n1 + n2 + ... + nk = n.
	\item \textbf{Theorem (Multinomial margins)}. If $X \sim Multk(n, p)$, then $Xj \sim Bin(n, p_j).$
	\item \textbf{Theorem (Multinomial lumping)}. If$X \sim Multk(n, p)$, then for any distinct i and j,$ Xi + Xj \sim Bin(n, pi + pj).$
	The random vector of counts obtained from merging categories i and j is still Multinomial. For example,	merging categories 1 and 2 gives
	$(X1 + X2, X3,... Xk) \sim Mult_{k1}(n, (p1 + p2, p3, ... , pk))$
	\item \textbf{Theorem (Multinomial conditioning)}. If$ X \sim Multk(n, p)$, then
	$(X2,..,Xk) | X1 = n1 \sim Mult_{k-1}(n - n1, (p2',...pk'));$
	where$p_j' = \frac{p_j}{p_2+...p_k}$
	\item \textcolor{red}{\textbf{Definition (Multivariate Normal distribution).}} A random vector X = (X1,...,Xk) is said to have a Multivariate Normal (MVN) distribution if \textbf{every} linear combination of the Xj has a Normal distribution. That is, we require t1X1 + t2X2 + ... + tkXk to have a Normal distribution for any t1, t2,..., tk in R. For k = 2 this distribution is called the Bivariate Normal (BVN). The marginal distribution of any component Xj of the random vector X is Normal which
	can be seen by taking tj = 1 and tj' = 0 for j, $!=$ j.
	\item Consider Z andW to be i.i.d. N(0, 1). Then (Z,W) is Bivariate Normal since the sum of independent Normals
	is also Normal. Furthermore, (Z + 2W, 3Z + 5W) is also Bivariate Normal since any linear combination of its	components can be expressed as a linear combination of Z and W:
	t1(Z + 2W) + t2(3Z + 5W) = (t1 + 3t2)Z + (2t1 + 5t2)W.
	\item Note that subsets of Multivariate normals are also multivariate normal. Further, combinations of multivariate normals which are \textbf{independent of one another} they are also multivariate normals jointly.
	\item these are described with a mean vector and variance covariance matrix
	\item\textbf{ Definition (Joint Moment Generating Function (MGF))}. The joint MGF of a random vector X = (X1,...,Xk)
	is the function which takes a vector of constants t' = (t1,...,tk) and returns	\\
	$M(t) = E[e^{t'X}] = E[e^{t_1X_1+...+t_kX_k}]\\
	$ With $ = e^{t_jE[X_j] + \frac{1}{2} t_j^2Var(X_j)}$\\
	$=> E[e^{t'X}] = E[e^{t_1X_1+...+t_kX_k}] = e^{t_1E[X_1] + ... t_kE[X_k] + \frac{1}{2} Var(t_1X_1+...t_kX_k)} $
	\item \textcolor{red}{\textbf{Theorem.}} Within a Multivariate Normal random vector, uncorrelated implies independence. That is, if
	X = (X1,X2) is Multivariate Normal, where X1 and X2 are subvectors, and every component of X1 is uncorrelated with every component of X2, then X1 and X2 are independent. In particular, if (X, Y) is	Bivariate Normal with Corr(X, Y) = 0, then X and Y are independent.
	
\end{enumerate}
\item Transformations
\begin{enumerate}
	\item \textcolor{red}{\textbf{Theorem (Change of variables).}} Let X = (X1,...,Xn) be a continuous random vector with joint PDF $f_X(x)$,
	and let Y = g(X) where g is an invertible function from $R^n to R^n$. Let y = g(x) which implies $x = g^{-1}(y)$. We
	consider the Jacobian matrix which is the matrix of all the partial derivatives $\frac{dx_i}{dy_j}$ that are assumed to exist
	and be continuous:\\
	$\frac{dX}{dY}=\begin{bmatrix}
	\frac{dx_1}{dy_1} & \frac{dx_1}{dy_2} & ... \frac{dx_1}{dy_n}\\
	\frac{dx_2}{dy_1} & \frac{dx_2}{dy_2} & ... \\
	\frac{dx_n}{dy_1} & ... &\frac{dx_n}{dy_n}
	\end{bmatrix}$\\
	and joint pdf $f_Y(y) = f_X(X)|\frac{dX}{dY}|$ and $|\frac{dX}{dY}| = |\frac{dY}{dX}|^{-1}$ 
	\item \textcolor{red}{\textbf{Example: Bivariate Normal joint PDF}}
	We find the PDF of the vector (Z,W) which follows a Bivariate Normal distribution with N(0, 1) marginals
	and $Corr(Z,W) = \rho \in(-1, 1)$. We construct (Z,W) by transforming another vector (X, Y) with $X, Y \sim
	N(0, 1)$, and X independent of Y as follows:
	$Z = X$
	$W = \rho X + \sqrt{1-\rho^2}Y;$
	The inverse transformation that maps (Z,W) into (X, Y) is
	$X = Z$
	$Y = -\frac{\rho}{\sqrt{1-\rho^2}}Z 	+ \frac{1}{\sqrt{1-\rho^2}}W$
	The Jacobian matrix is
	$\frac{d(x,y)}{d(z,w)} = \begin{bmatrix}
		1 & 0 \\
		-\frac{\rho}{\sqrt{1-\rho^2}} & \frac{1}{\sqrt{1-\rho^2}}
	\end{bmatrix}$\\
	$=>f_{Z,W}=f_(x,y)(x,y)* |\frac{d(x,y)}{d(z,w)}| = f_(x,y)(x,y)* \frac{1}{\sqrt{1-\rho^2}} $
	\item \textcolor{red}{\textbf{Theorem (Convolution sums and integrals). }}If X and Y are independent discrete random variables, then
	the PMF of their sum T = X + Y is
	$P(T = t) =\sum_{x}P(Y=t-x)P(X=x) = \sum_{y}P(X=t-y)P(Y=y)$ \\
	and\\
	$f_T(t) = \int_{-\infty}^{\infty}f_y(t-x)f_x(x)dx = \int_{-\infty}^{\infty}f_x(t-y)f_y(y)dx$
	\item \textbf{Example: Uniform convolution}
	Let X, Y be i.i.d. Unif(0; 1). Find the distribution of T = X + Y.
	Solution: The PDF of X and of Y is $g(x) = 1 if x \in (0, 1) and g(x) = 0$, otherwise. The convolution formula gives:\\
	$\int_{-\infty}^{\infty}f_y(t-x)f(x)dx =\int_{-\infty}^{\infty}g(t-x)g(x)dx  $\\
	we can see that we must have \\
	$0<t-x<1 \& 0<x<1 => x<t<1+x \& 0 < x < 1 => 0<x<t<1+x<2$ Depending on the value of t, $t<1$ or $t\ge1$ x falls in either $0<x<t$ or $t-1<x<1$\\
	$=> f_T(t) = \begin{cases}
	\int_{0}^{t}dx = t & 0<t\le1\\
	\int_{t-1}^{1}dx = t & 1<t\le2	
	\end{cases}$\\
	\item \textcolor{red}{\textbf{Beta Distribution }} A random variable X is said to have the Beta distribution with parameters
	$a > 0 $and $b > 0$ if its PDF is\\
	$\frac{1}{\beta(a,b)} x^{a-1}(1-x)^{b-1} \, \, 0<x<1$
	\item $\Gamma(a) = (a-1)!$
	\item \textbf{Definition (Gamma distribution).} A random variable Y is said to have the Gamma distribution with parameters	a and $\lambda$, where $a > 0$ and $\lambda > 0$, if its PDF is\\
	$f(y) = \frac{1}{\Gamma(a)}(\lambda y)^ae^{-\lambda y}\frac{1}{y}$
	\item The general $Gamma(a,\lambda)$ distribution can be constructed from the $X \sim Gamma(a, 1)$ by a scale transformation. \\
	Consider the random variable $Y = \frac{X}{\lambda}$ for some $\lambda > 0$. By the change of variables formula with $x = \lambda y$ and
	$\frac{dx}{dy} = \lambda$, the PDF of Y is \\
	$f_y(y) = f_x(x)|\frac{dx}{dy}| = \frac{1}{\Gamma(a)}(x)^ae^{-x}\frac{1}{x} * \lambda $ \\
	substituting for $x=\lambda y$ and cancelling we get\\ $\frac{1}{\Gamma(a)}(\lambda y)^ae^{-\lambda y}\frac{1}{y}$ 
\end{enumerate}
\item Conditional Expectation:
\begin{enumerate}
	\item \textbf{Definition (Conditional expectation given an event).} Let A be an event with positive probability, P(A) > 0. 
	\\
	Discrete: $E[X|A] = \sum_{x\in X} x*P(X=x|A)$\\
	Continuous $E[X|A] = \int_{-\infty}xf_x(x|A)dx$ with $f_x(x|A) = \frac{d}{dx} P(X\le x|A) = \frac{d}{dx} F(x|A)$
	\\
	\textcolor{red}{\textbf{Using Bayes Rule}}
	$f(x|A) = \frac{P(A|x=x)f_x(x)}{P(A)}$
	\item T\textbf{heorem(Law of total expectation). }Let A1,A2,...,An be a partition of a sample space,with P(Ai) > 0 for all i = 1,2,...,n. Let Y be a random variable on this sample space. Then $E(Y) = \sum_{i=1}^{n}E(Y | Ai)P(Ai). $
	\\
	The law of total probability is a specific case of this law using indicator random variables and the fundadmental bridge.
	\item Conditional Expectations are functions of the conditioned variable.\\
	$E[Y|X] = g(X)$. thus we can ask for $E[g(x)] Var(g(X)) $ ...
	\item \textcolor{red}{\textbf{Theorem(Taking out what is known)}}. For any function h(·), we have E(h(X)Y | X) = h(X)E(Y | X).\\
	Note that the above equality means that the random variable g1(X) = E(h(X)Y | X) is equal with the random variable g2(X) = h(X)E(Y | X). 
	\item Linearity : $E[X_1+X_2..X_n|Y] = E[X_1|Y] + E[X_2|Y] .. E[X_n|Y]$
	\item Adams law: $E[E[X|Y]] = E[X]$
	\item Adams Law w/Extra conditioning : $E[E[Y|X,Z]|Z] = E[Y|Z]$
	\item \textcolor{red}{\textbf{Theorem(Projection interpretation).}} For any function $h(·)$,the random variable $Y-E[Y|X]$ is uncorrelated with $h(X)$. Since $E(Y-E(Y|X)) = E(Y)-E(E(Y|X)) = E(Y)-E(Y) = 0$, this is equivalent with $E((Y-E(Y|X))h(X)) = 0$. Main point being that $E[Y|X]$ minimizes the function $|Y - E[Y|X]|$ 
	\item \textcolor{red}{\textbf{Definition(Conditional variance).}} The conditional variance of Y given X is $Var(Y | X) = E[[Y-E(Y|X)]^2|X]$
	\\
	This is equivalent to $Var(Y|X) = E(Y^2|X)-(E(Y|X))^2$ .
	\item \textcolor{red}{\textbf{Theorem(Eve’slaw: connecting conditional variance to unconditional variance)}}. For any random variables X and Y, we have $Var(Y) = E(Var(Y | X)) + Var(E(Y | X))$. This relation is known as the law of total variance, or as the variance decomposition formula.
	\\
	Aka there is variance within a group, and there is a variance between groups 
\end{enumerate}
\end{enumerate}
\text{Happy holidays!}
\end{document}
