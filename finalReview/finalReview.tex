\documentclass[11pt]{article}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb}
%\usepackage[dvips]{graphicx,color}
\usepackage{graphicx,color}
\usepackage{graphicx,color,bm}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{float}

%\setlength{\oddsidemargin}{0.1 in} \setlength{\evensidemargin}{-0.1
%in} \setlength{\topmargin}{-0.6 in} \setlength{\textwidth}{6.5 in}
%\setlength{\textheight}{8.5 in} \setlength{\headsep}{0.75 in}
%\setlength{\parindent}{0 in} \setlength{\parskip}{0.1 in}

\textwidth 6.3in \textheight 8.8in \topmargin -0.5truein
\oddsidemargin .15truein
\parskip .1in
\renewcommand{\baselinestretch}{1.53}  % double spaced


\newcommand{\homework}[9]{
	\pagestyle{myheadings}
	\thispagestyle{plain}
	\newpage
	\setcounter{page}{1}
	\noindent
	\begin{center}
		\framebox{
			\vbox{\vspace{2mm}
				\hbox to 6.28in { {\bf Math531:~Regression - I  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\Large \hfill #1 (#2)  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\it Instructor: #3 \hfill} }
				\hbox to 6.28in { {\it Office hours: #4  \hfill #6}}
				\vspace{2mm}}
		}
	\end{center}
	\markboth{#1}{#1}
	\vspace*{4mm}
}

% ----------------------- MATH -------------------------
\def\av{\boldsymbol a}
\def\bv{\boldsymbol b}
\def\cv{\boldsymbol c}
\def\dv{\boldsymbol d}
\def\ev{\boldsymbol e}
\def\fv{\boldsymbol f}
\def\gv{\boldsymbol g}
\def\hv{\boldsymbol h}
\def\iv{\boldsymbol i}
\def\gv{\boldsymbol j}
\def\kv{\boldsymbol k}
\def\lv{\boldsymbol l}
\def\mv{\boldsymbol m}
\def\nv{\boldsymbol n}
\def\ov{\boldsymbol o}
\def\pv{\boldsymbol p}
\def\qv{\boldsymbol q}
\def\rv{\boldsymbol r}
\def\sv{\boldsymbol s}
\def\tv{\boldsymbol t}
\def\uv{\boldsymbol u}
\def\vv{\boldsymbol v}
\def\wv{\boldsymbol w}
\def\xv{\boldsymbol x}
\def\yv{\boldsymbol y}
\def\zv{\boldsymbol z}
\def\Av{\boldsymbol A}
\def\Bv{\boldsymbol B}
\def\Cv{\boldsymbol C}
\def\Dv{\boldsymbol D}
\def\Ev{\boldsymbol E}
\def\Fv{\boldsymbol F}
\def\Gv{\boldsymbol G}
\def\Hv{\boldsymbol H}
\def\Iv{\boldsymbol I}
\def\Gv{\boldsymbol J}
\def\Kv{\boldsymbol K}
\def\Lv{\boldsymbol L}
\def\Mv{\boldsymbol M}
\def\Nv{\boldsymbol N}
\def\Ov{\boldsymbol O}
\def\Pv{\boldsymbol P}
\def\Qv{\boldsymbol Q}
\def\Rv{\boldsymbol R}
\def\Sv{\boldsymbol S}
\def\Tv{\boldsymbol T}
\def\Uv{\boldsymbol U}
\def\Vv{\boldsymbol V}
\def\Wv{\boldsymbol W}
\def\Xv{\boldsymbol X}
\def\Yv{\boldsymbol Y}
\def\Zv{\boldsymbol Z}
\def\Abf{\mathbf A}
\def\Bbf{\mathbf B}
\def\Cbf{\mathbf C}
\def\Dbf{\mathbf D}
\def\Ebf{\mathbf E}
\def\Fbf{\mathbf F}
\def\Gbf{\mathbf G}
\def\Hbf{\mathbf H}
\def\Ibf{\mathbf I}
\def\Gbf{\mathbf J}
\def\Kbf{\mathbf K}
\def\Lbf{\mathbf L}
\def\Mbf{\mathbf M}
\def\Nbf{\mathbf N}
\def\Obf{\mathbf O}
\def\Pbf{\mathbf P}
\def\Qbf{\mathbf Q}
\def\Rbf{\mathbf R}
\def\Sbf{\mathbf S}
\def\Tbf{\mathbf T}
\def\Ubf{\mathbf U}
\def\Vbf{\mathbf V}
\def\Wbf{\mathbf W}
\def\Xbf{\mathbf X}
\def\Ybf{\mathbf Y}
\def\Jbf{\mathbf J}
\def\Zbf{\mathbf Z}
\def\Am{\mathrm A}
\def\Bm{\mathrm B}
\def\Cm{\mathrm C}
\def\Dm{\mathrm D}
\def\Em{\mathrm E}
\def\Fm{\mathrm F}
\def\Gm{\mathrm G}
\def\Hm{\mathrm H}
\def\Im{\mathrm I}
\def\Gm{\mathrm J}
\def\Km{\mathrm K}
\def\Lm{\mathrm L}
\def\Mm{\mathrm M}
\def\Nm{\mathrm N}
\def\Om{\mathrm O}
\def\Pm{\mathrm P}
\def\Qm{\mathrm Q}
\def\Rm{\mathrm R}
\def\Sm{\mathrm S}
\def\Tm{\mathrm T}
\def\Um{\mathrm U}
\def\mv{\mathrm V}
\def\Wm{\mathrm W}
\def\Xm{\mathrm X}
\def\Ym{\mathrm Y}
\def\Zm{\mathrm Z}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}
\newcommand{\alphav}{\mbox{\boldmath{$\alpha$}}}
\newcommand{\betav}{\mbox{\boldmath{$\beta$}}}
\newcommand{\gammav}{\mbox{\boldmath{$\gamma$}}}
\newcommand{\deltav}{\mbox{\boldmath{$\delta$}}}
\newcommand{\epsilonv}{\mbox{\boldmath{$\epsilon$}}}
\newcommand{\zetav}{\mbox{\boldmath$\zeta$}}
\newcommand{\etav}{\mbox{\boldmath{$\eta$}}}
\newcommand{\iotav}{\mbox{\boldmath{$\iota$}}}
\newcommand{\kappav}{\mbox{\boldmath{$\kappa$}}}
\newcommand{\lambdav}{\mbox{\boldmath{$\lambda$}}}
\newcommand{\muv}{\mbox{\boldmath{$\mu$}}}
\newcommand{\nuv}{\mbox{\boldmath{$\nu$}}}
\newcommand{\xiv}{\mbox{\boldmath{$\xi$}}}
\newcommand{\omicronv}{\mbox{\boldmath{$\omicron$}}}
\newcommand{\piv}{\mbox{\boldmath{$\pi$}}}
\newcommand{\rhov}{\mbox{\boldmath{$\rho$}}}
\newcommand{\sigmav}{\mbox{\boldmath{$\sigma$}}}
\newcommand{\tauv}{\mbox{\boldmath{$\tau$}}}
\newcommand{\upsilonv}{\mbox{\boldmath{$\upsilon$}}}
\newcommand{\phiv}{\mbox{\boldmath{$\phi$}}}
\newcommand{\varphiv}{\mbox{\boldmath{$\varphi$}}}
\newcommand{\chiv}{\mbox{\boldmath{$\chi$}}}
\newcommand{\psiv}{\mbox{\boldmath{$\psi$}}}
\newcommand{\omegav}{\mbox{\boldmath{$\omega$}}}
\newcommand{\Sigmav}{\mbox{\boldmath{$\Sigma$}}}
\newcommand{\Lambdav}{\mbox{\boldmath{$\Lambda$}}}
\newcommand{\Deltav}{\mbox{\boldmath{$\Delta$}}}
\newcommand{\Omegav}{\mbox{\boldmath{$\Omega$}}}
\newcommand{\varepsilonv}{\mbox{\boldmath{$\varepsilon$}}}

\newcommand{\eps}{\varepsilon}
\newcommand{\epsv}{\mbox{\boldmath{$\varepsilon$}}}

\def\1v{\mathbf 1}
\def\0v{\mathbf 0}
\def\Id{\mathbf I} % identity matrix
\newcommand{\ind}[1]{\mathbbm{1}_{\left[ {#1} \right] }}
\newcommand{\Ind}[1]{\mathbbm{1}_{\left\{ {#1} \right\} }}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\QED}{\begin{flushright} {\bf QED} \end{flushright}}
\newcommand{\R}{\mathbb R}
\newcommand{\Real}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\E}{\mathbb E}
\newcommand{\sgn}{\mathop{\mathrm{sign}}}
\def\Pr{\mathrm P}
\def\pr{\mathrm P}
\newcommand{\Var}{\mathop{\rm Var}}
\newcommand{\var}{\mathop{\rm Var}}
\newcommand{\Cov}{\mathop{\rm Cov}}
\newcommand{\cov}{\mathop{\rm Cov}}
\newcommand{\Corr}{\mathop{\rm Corr}}
\newcommand{\ang}{\mathop{\rm Angle}}
\newcommand{\tr}{\mathop{\rm trace}}
\newcommand{\proj}{\mathop{\rm Proj}}
\newcommand{\rank}{\mathop{\rm rank}}

\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\Diag}{\mathop{\rm diag}}
\newcommand{\sk}{\vspace{0.5cm}}
\newcommand{\ds}{\displaystyle}
\newcommand{\mb}{\mbox}
\newcommand{\wh}{\widehat}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\To}{\longrightarrow}

\def\equalLaw{\stackrel{\mathcal{L}}{=}}
\def\equallaw{\stackrel{\mathcal{L}}{=}}

\def\half{\frac{1}{2}}

\usepackage{caption}

\begin{document}

\begin{title}
	{\Large\bf Final Review}
\end{title}

\author{\bf Alexander Van Roijen}

\maketitle

\newpage
Please complete the following:
\begin{enumerate}
\item Lecture 1. Basics of Probability
\begin{enumerate}
	\item \textbf{Definition (Sample space and event).} The sample space S of an experiment is the set of all possible outcomes
	of the experiment. An event A is a subset of the sample space S , and we say that A occurred if the
	actual outcome is in A.
	\item \textbf{Definition (General definition of probability).} A probability space consists of a sample space S and a
	probability function P(.) which takes an event A $\subset$ S as input and returns P(A), a real number between 0
	and 1, as output. The probability function must satisfy the following axioms: $P(\emptyset) = 0 \, P(S)=1$ and for a union of disjoint events, we get $P(A_1\cup...A_n) = P(A_1)+...P(A_n)$ 
	\item \textbf{Theorem. Properties of probability.} A probability function has the following properties, for any events A
	and B.
	\begin{enumerate}
		\item $P(A^c)=1-P(A)$
		\item if $A\subset B P(A)\le P(B)$
		\item $P(A\cup B) = P(A)+P(B) - P(A\cap B)$ \text{ which can be extended}\\
		$P(A\cup B \cup C) = P(A) + P(B) + P(C) - P(A\cap B) - P(A \cap C) - P(B \cap C) - P(A \cap B \cap C)$
	\end{enumerate}
	\item\textbf{ Definition (Conditional probability).} If A and B are events with $P(B) > 0$, then the conditional probability
	of A given B, denoted by $P(A|B)$, is defined as:
	$P(A | B) =\frac{
	P(A \cap B)}
	{P(B)}$
	Further note that all probabilities are in fact conditional. We like to think of P(A) as our prior beliefs of an event, and P(A|B) as our posterior, or what we think it is given something is already known.
	\item \textbf{Theorem.} For any events A1,..An with positive probabilities,\\
	$P(A1,...An) = P(A1)P(A2|A1)P(A3|A1,A2)...P(An|A1,...A_{n-1})$
	\item\textbf{ Theorem (Bayes’ rule).}
	$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
	\item \textcolor{red}{\textbf{Theorem (Law of total probability (LOTP)).}} Let A1,...An be a partition of the sample space S with
	$P(Ai) > 0$ for all i. Then\\
	$P(B) = \sum_{i=1}^{n}P(B|A_i)P(A_i)$\\
	When we condition on an event E, we update our beliefs
	to be consistent with this knowledge, effectively putting ourselves in a universe where we know that E
	occurred. Within our new universe, however, the laws of probability operate just as before. Conditional
	probability satisfies all the properties of probability!
	\item\textbf{ Theorem (Bayes’ rule with extra conditioning). }Provided that $P(A\cap E) > 0$ and $P(B \cap E) > 0$, we have
	$P(A | B, E) =
	\frac{P(B | A, E)P(A | E)}
	{P(B | E)}$
	\item T\textbf{heorem (Law of total probability (LOTP) with extra conditioning).} Let A1... An be a partition of the
	sample space S with $P(Ai | E) > 0$ for all i. Then \\
	$P(B|E) = \sum_{i=1}^{n}P(B|A_i,E)P(A_i|E)$
	\item\textbf{ Definition (Independence of two events).} Events A and B are independent if P(A \ B) = P(A)P(B). If
	P(A) > 0 and P(B) > 0, then this is equivalent with P(A j B) = P(A), and also equivalent with P(B j A) =
	P(B). Independence is a symmetric relation.
	\item \textbf{Proposition.} If A and B are independent, then $A^c$ and $B$ are independent, $A^c$ and $B^c$ are independent, and
	A and $B^c$ are independent.
	\item Definition (Independence of three events). Events A, B and C are said to be independent if all of the
	following relations hold:\\
	$P(A \cap B) = P(A)P(B);\\
	P(A \cap C) = P(A)P(C);\\
	P(B \cap C) = P(B)P(C);\\
	P(A \cap B \cap C) = P(A)P(B)P(C)$
	\item \textbf{Definition (Conditional independence).} Events A and B are said to be conditionally independent given E if
	$P(A \cap B | E) = P(A | E)P(B | E).$
	\item Problems shown: Monty Hall, and Positive test of conditionitis and bayes rule
\end{enumerate}
\item
\begin{enumerate}
	\item \textbf{Definition (Random variable).} Given an experiment with sample space S , a random variable is a function
	from the sample space S to the real numbers R. It is common, but not required, to denote random variables
	by capital letters. P(X=x) = P(X=X(s))
	\item Discrete PMFs are non negative, and sum to one over their support.
	\item \textbf{Definition (Bernoulli distribution). }An random variable X is said to have a Bernoulli distribution with
	parameter p if P(X = 1) = p and P(X = 0) = 1 $\sim$ p, where $0 < p < 1$. We write this as X $\sim$ Bern(p).
	\item \textbf{Theorem. }Let X $\sim Bin(n, p)$, and $q = 1 - p$ (often taken to denote the failure of a Bernoulli trial). Then
	$n - X \sim Bin(n, q)$.
	\item \textbf{Theorem (Hypergeometric PMF).} Consider an urn with w white balls and b blacks balls. We draw n balls
	out of the urn at random without replacement such that all the
	$\binom{w+b}{n}$
	samples are equally likely. Let X be the
	number of white balls in the sample. Then X is said to have the Hypergeometric distribution with parameters
	w, b and n: $X \sim HGeom(w, b, n)$. Then the PMF of X is\\
	$P(X=k)=\frac{\binom{w}{k}\binom{b}{n-k}}{\binom{b+w}{n}}$
	\item \textcolor{red}{\textbf{Theorem.}} If $X \sim HGeom(w, b, n)$ and $Y \sim HGeom(n,w+b-n,w)$, then X and Y have the same distribution.
	\item \textbf{Theorem.} If X $\sim$ Bin(n,p), Y $\sim$ Bin(m,p), and X is independent of Y, then the conditional distribution of
	$X|X+Y=r \sim$ hgeom(n,m,r).
	\item \textbf{Theorem (Binomial as a limiting case of the Hypergeometric).} If X$\sim$ HGeom(w, b, n) and N = w+b approaches infinity
	such that p = w/(w + b) remains fixed, then the PMF of X converges to the Bin(n, p) PMF.
	\item \textbf{ Theorem (PMF of g(X))}. Let X be a discrete random variable and $g R->R$. Then the support of g(X) is
	the set of all y such that g(x) = y for at least one x in the support of X, and the PMF of g(X) is
	$P(g(X) = Y) = \sum_{x:g(x)=y} P(X=x)$
	\item \textbf{Definition (Independence of two random variables).} Random variables X and Y are said to be independent
	if
	$P(X \le x, Y \le y) = P(X \le x)P(Y \le y)$
	for all $x, y \in R$. In the discrete case, this is equivalent to the condition
	P(X = x, Y = y) = P(X = x)P(Y = y);
	for all x in the support of X and all y in the support of Y.
\end{enumerate}

\end{enumerate}
\text{Happy holidays!}
\end{document}
