\documentclass[11pt]{article}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb}
%\usepackage[dvips]{graphicx,color}
\usepackage{graphicx,color}
\usepackage{graphicx,color,bm}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{float}

%\setlength{\oddsidemargin}{0.1 in} \setlength{\evensidemargin}{-0.1
%in} \setlength{\topmargin}{-0.6 in} \setlength{\textwidth}{6.5 in}
%\setlength{\textheight}{8.5 in} \setlength{\headsep}{0.75 in}
%\setlength{\parindent}{0 in} \setlength{\parskip}{0.1 in}

\textwidth 6.3in \textheight 8.8in \topmargin -0.5truein
\oddsidemargin .15truein
\parskip .1in
\renewcommand{\baselinestretch}{1.53}  % double spaced


\newcommand{\homework}[9]{
	\pagestyle{myheadings}
	\thispagestyle{plain}
	\newpage
	\setcounter{page}{1}
	\noindent
	\begin{center}
		\framebox{
			\vbox{\vspace{2mm}
				\hbox to 6.28in { {\bf Math531:~Regression - I  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\Large \hfill #1 (#2)  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\it Instructor: #3 \hfill} }
				\hbox to 6.28in { {\it Office hours: #4  \hfill #6}}
				\vspace{2mm}}
		}
	\end{center}
	\markboth{#1}{#1}
	\vspace*{4mm}
}

% ----------------------- MATH -------------------------
\def\av{\boldsymbol a}
\def\bv{\boldsymbol b}
\def\cv{\boldsymbol c}
\def\dv{\boldsymbol d}
\def\ev{\boldsymbol e}
\def\fv{\boldsymbol f}
\def\gv{\boldsymbol g}
\def\hv{\boldsymbol h}
\def\iv{\boldsymbol i}
\def\gv{\boldsymbol j}
\def\kv{\boldsymbol k}
\def\lv{\boldsymbol l}
\def\mv{\boldsymbol m}
\def\nv{\boldsymbol n}
\def\ov{\boldsymbol o}
\def\pv{\boldsymbol p}
\def\qv{\boldsymbol q}
\def\rv{\boldsymbol r}
\def\sv{\boldsymbol s}
\def\tv{\boldsymbol t}
\def\uv{\boldsymbol u}
\def\vv{\boldsymbol v}
\def\wv{\boldsymbol w}
\def\xv{\boldsymbol x}
\def\yv{\boldsymbol y}
\def\zv{\boldsymbol z}
\def\Av{\boldsymbol A}
\def\Bv{\boldsymbol B}
\def\Cv{\boldsymbol C}
\def\Dv{\boldsymbol D}
\def\Ev{\boldsymbol E}
\def\Fv{\boldsymbol F}
\def\Gv{\boldsymbol G}
\def\Hv{\boldsymbol H}
\def\Iv{\boldsymbol I}
\def\Gv{\boldsymbol J}
\def\Kv{\boldsymbol K}
\def\Lv{\boldsymbol L}
\def\Mv{\boldsymbol M}
\def\Nv{\boldsymbol N}
\def\Ov{\boldsymbol O}
\def\Pv{\boldsymbol P}
\def\Qv{\boldsymbol Q}
\def\Rv{\boldsymbol R}
\def\Sv{\boldsymbol S}
\def\Tv{\boldsymbol T}
\def\Uv{\boldsymbol U}
\def\Vv{\boldsymbol V}
\def\Wv{\boldsymbol W}
\def\Xv{\boldsymbol X}
\def\Yv{\boldsymbol Y}
\def\Zv{\boldsymbol Z}
\def\Abf{\mathbf A}
\def\Bbf{\mathbf B}
\def\Cbf{\mathbf C}
\def\Dbf{\mathbf D}
\def\Ebf{\mathbf E}
\def\Fbf{\mathbf F}
\def\Gbf{\mathbf G}
\def\Hbf{\mathbf H}
\def\Ibf{\mathbf I}
\def\Gbf{\mathbf J}
\def\Kbf{\mathbf K}
\def\Lbf{\mathbf L}
\def\Mbf{\mathbf M}
\def\Nbf{\mathbf N}
\def\Obf{\mathbf O}
\def\Pbf{\mathbf P}
\def\Qbf{\mathbf Q}
\def\Rbf{\mathbf R}
\def\Sbf{\mathbf S}
\def\Tbf{\mathbf T}
\def\Ubf{\mathbf U}
\def\Vbf{\mathbf V}
\def\Wbf{\mathbf W}
\def\Xbf{\mathbf X}
\def\Ybf{\mathbf Y}
\def\Jbf{\mathbf J}
\def\Zbf{\mathbf Z}
\def\Am{\mathrm A}
\def\Bm{\mathrm B}
\def\Cm{\mathrm C}
\def\Dm{\mathrm D}
\def\Em{\mathrm E}
\def\Fm{\mathrm F}
\def\Gm{\mathrm G}
\def\Hm{\mathrm H}
\def\Im{\mathrm I}
\def\Gm{\mathrm J}
\def\Km{\mathrm K}
\def\Lm{\mathrm L}
\def\Mm{\mathrm M}
\def\Nm{\mathrm N}
\def\Om{\mathrm O}
\def\Pm{\mathrm P}
\def\Qm{\mathrm Q}
\def\Rm{\mathrm R}
\def\Sm{\mathrm S}
\def\Tm{\mathrm T}
\def\Um{\mathrm U}
\def\mv{\mathrm V}
\def\Wm{\mathrm W}
\def\Xm{\mathrm X}
\def\Ym{\mathrm Y}
\def\Zm{\mathrm Z}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}
\newcommand{\alphav}{\mbox{\boldmath{$\alpha$}}}
\newcommand{\betav}{\mbox{\boldmath{$\beta$}}}
\newcommand{\gammav}{\mbox{\boldmath{$\gamma$}}}
\newcommand{\deltav}{\mbox{\boldmath{$\delta$}}}
\newcommand{\epsilonv}{\mbox{\boldmath{$\epsilon$}}}
\newcommand{\zetav}{\mbox{\boldmath$\zeta$}}
\newcommand{\etav}{\mbox{\boldmath{$\eta$}}}
\newcommand{\iotav}{\mbox{\boldmath{$\iota$}}}
\newcommand{\kappav}{\mbox{\boldmath{$\kappa$}}}
\newcommand{\lambdav}{\mbox{\boldmath{$\lambda$}}}
\newcommand{\muv}{\mbox{\boldmath{$\mu$}}}
\newcommand{\nuv}{\mbox{\boldmath{$\nu$}}}
\newcommand{\xiv}{\mbox{\boldmath{$\xi$}}}
\newcommand{\omicronv}{\mbox{\boldmath{$\omicron$}}}
\newcommand{\piv}{\mbox{\boldmath{$\pi$}}}
\newcommand{\rhov}{\mbox{\boldmath{$\rho$}}}
\newcommand{\sigmav}{\mbox{\boldmath{$\sigma$}}}
\newcommand{\tauv}{\mbox{\boldmath{$\tau$}}}
\newcommand{\upsilonv}{\mbox{\boldmath{$\upsilon$}}}
\newcommand{\phiv}{\mbox{\boldmath{$\phi$}}}
\newcommand{\varphiv}{\mbox{\boldmath{$\varphi$}}}
\newcommand{\chiv}{\mbox{\boldmath{$\chi$}}}
\newcommand{\psiv}{\mbox{\boldmath{$\psi$}}}
\newcommand{\omegav}{\mbox{\boldmath{$\omega$}}}
\newcommand{\Sigmav}{\mbox{\boldmath{$\Sigma$}}}
\newcommand{\Lambdav}{\mbox{\boldmath{$\Lambda$}}}
\newcommand{\Deltav}{\mbox{\boldmath{$\Delta$}}}
\newcommand{\Omegav}{\mbox{\boldmath{$\Omega$}}}
\newcommand{\varepsilonv}{\mbox{\boldmath{$\varepsilon$}}}

\newcommand{\eps}{\varepsilon}
\newcommand{\epsv}{\mbox{\boldmath{$\varepsilon$}}}

\def\1v{\mathbf 1}
\def\0v{\mathbf 0}
\def\Id{\mathbf I} % identity matrix
\newcommand{\ind}[1]{\mathbbm{1}_{\left[ {#1} \right] }}
\newcommand{\Ind}[1]{\mathbbm{1}_{\left\{ {#1} \right\} }}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\QED}{\begin{flushright} {\bf QED} \end{flushright}}
\newcommand{\R}{\mathbb R}
\newcommand{\Real}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\E}{\mathbb E}
\newcommand{\sgn}{\mathop{\mathrm{sign}}}
\def\Pr{\mathrm P}
\def\pr{\mathrm P}
\newcommand{\Var}{\mathop{\rm Var}}
\newcommand{\var}{\mathop{\rm Var}}
\newcommand{\Cov}{\mathop{\rm Cov}}
\newcommand{\cov}{\mathop{\rm Cov}}
\newcommand{\Corr}{\mathop{\rm Corr}}
\newcommand{\ang}{\mathop{\rm Angle}}
\newcommand{\tr}{\mathop{\rm trace}}
\newcommand{\proj}{\mathop{\rm Proj}}
\newcommand{\rank}{\mathop{\rm rank}}

\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\Diag}{\mathop{\rm diag}}
\newcommand{\sk}{\vspace{0.5cm}}
\newcommand{\ds}{\displaystyle}
\newcommand{\mb}{\mbox}
\newcommand{\wh}{\widehat}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\To}{\longrightarrow}

\def\equalLaw{\stackrel{\mathcal{L}}{=}}
\def\equallaw{\stackrel{\mathcal{L}}{=}}

\def\half{\frac{1}{2}}

\usepackage{caption}

\begin{document}

\begin{title}
	{\Large\bf Final Review}
\end{title}

\author{\bf Alexander Van Roijen}

\maketitle

\newpage
Please complete the following:
\begin{enumerate}
\item Lecture 1. Basics of Probability
\begin{enumerate}
	\item \textbf{Definition (Sample space and event).} The sample space S of an experiment is the set of all possible outcomes
	of the experiment. An event A is a subset of the sample space S , and we say that A occurred if the
	actual outcome is in A.
	\item \textbf{Definition (General definition of probability).} A probability space consists of a sample space S and a
	probability function P(.) which takes an event A $\subset$ S as input and returns P(A), a real number between 0
	and 1, as output. The probability function must satisfy the following axioms: $P(\emptyset) = 0 \, P(S)=1$ and for a union of disjoint events, we get $P(A_1\cup...A_n) = P(A_1)+...P(A_n)$ 
	\item \textbf{Theorem. Properties of probability.} A probability function has the following properties, for any events A
	and B.
	\begin{enumerate}
		\item $P(A^c)=1-P(A)$
		\item if $A\subset B P(A)\le P(B)$
		\item $P(A\cup B) = P(A)+P(B) - P(A\cap B)$ \text{ which can be extended}\\
		$P(A\cup B \cup C) = P(A) + P(B) + P(C) - P(A\cap B) - P(A \cap C) - P(B \cap C) - P(A \cap B \cap C)$
	\end{enumerate}
	\item\textbf{ Definition (Conditional probability).} If A and B are events with $P(B) > 0$, then the conditional probability
	of A given B, denoted by $P(A|B)$, is defined as:
	$P(A | B) =\frac{
	P(A \cap B)}
	{P(B)}$
	Further note that all probabilities are in fact conditional. We like to think of P(A) as our prior beliefs of an event, and P(A|B) as our posterior, or what we think it is given something is already known.
	\item \textbf{Theorem.} For any events A1,..An with positive probabilities,\\
	$P(A1,...An) = P(A1)P(A2|A1)P(A3|A1,A2)...P(An|A1,...A_{n-1})$
	\item\textbf{ Theorem (Bayes’ rule).}
	$P(A|B) = \frac{P(B|A)P(A)}{P(B)}$
	\item \textcolor{red}{\textbf{Theorem (Law of total probability (LOTP)).}} Let A1,...An be a partition of the sample space S with
	$P(Ai) > 0$ for all i. Then\\
	$P(B) = \sum_{i=1}^{n}P(B|A_i)P(A_i)$\\
	When we condition on an event E, we update our beliefs
	to be consistent with this knowledge, effectively putting ourselves in a universe where we know that E
	occurred. Within our new universe, however, the laws of probability operate just as before. Conditional
	probability satisfies all the properties of probability!
	\item\textbf{ Theorem (Bayes’ rule with extra conditioning). }Provided that $P(A\cap E) > 0$ and $P(B \cap E) > 0$, we have
	$P(A | B, E) =
	\frac{P(B | A, E)P(A | E)}
	{P(B | E)}$
	\item T\textbf{heorem (Law of total probability (LOTP) with extra conditioning).} Let A1... An be a partition of the
	sample space S with $P(Ai | E) > 0$ for all i. Then \\
	$P(B|E) = \sum_{i=1}^{n}P(B|A_i,E)P(A_i|E)$
	\item\textbf{ Definition (Independence of two events).} Events A and B are independent if P(A \ B) = P(A)P(B). If
	P(A) > 0 and P(B) > 0, then this is equivalent with P(A j B) = P(A), and also equivalent with P(B j A) =
	P(B). Independence is a symmetric relation.
	\item \textbf{Proposition.} If A and B are independent, then $A^c$ and $B$ are independent, $A^c$ and $B^c$ are independent, and
	A and $B^c$ are independent.
	\item Definition (Independence of three events). Events A, B and C are said to be independent if all of the
	following relations hold:\\
	$P(A \cap B) = P(A)P(B);\\
	P(A \cap C) = P(A)P(C);\\
	P(B \cap C) = P(B)P(C);\\
	P(A \cap B \cap C) = P(A)P(B)P(C)$
	\item \textbf{Definition (Conditional independence).} Events A and B are said to be conditionally independent given E if
	$P(A \cap B | E) = P(A | E)P(B | E).$
	\item Problems shown: Monty Hall, and Positive test of conditionitis and bayes rule
\end{enumerate}
\item
\begin{enumerate}
	\item \textbf{Definition (Random variable).} Given an experiment with sample space S , a random variable is a function
	from the sample space S to the real numbers R. It is common, but not required, to denote random variables
	by capital letters. P(X=x) = P(X=X(s))
	\item Discrete PMFs are non negative, and sum to one over their support.
	\item \textbf{Definition (Bernoulli distribution). }An random variable X is said to have a Bernoulli distribution with
	parameter p if P(X = 1) = p and P(X = 0) = 1 $\sim$ p, where $0 < p < 1$. We write this as X $\sim$ Bern(p).
	\item \textbf{Theorem. }Let X $\sim Bin(n, p)$, and $q = 1 - p$ (often taken to denote the failure of a Bernoulli trial). Then
	$n - X \sim Bin(n, q)$.
	\item \textbf{Theorem (Hypergeometric PMF).} Consider an urn with w white balls and b blacks balls. We draw n balls
	out of the urn at random without replacement such that all the
	$\binom{w+b}{n}$
	samples are equally likely. Let X be the
	number of white balls in the sample. Then X is said to have the Hypergeometric distribution with parameters
	w, b and n: $X \sim HGeom(w, b, n)$. Then the PMF of X is\\
	$P(X=k)=\frac{\binom{w}{k}\binom{b}{n-k}}{\binom{b+w}{n}}$
	\item \textcolor{red}{\textbf{Theorem.}} If $X \sim HGeom(w, b, n)$ and $Y \sim HGeom(n,w+b-n,w)$, then X and Y have the same distribution.
	\item \textbf{Theorem.} If X $\sim$ Bin(n,p), Y $\sim$ Bin(m,p), and X is independent of Y, then the conditional distribution of
	$X|X+Y=r \sim$ hgeom(n,m,r).
	\item \textbf{Theorem (Binomial as a limiting case of the Hypergeometric).} If X$\sim$ HGeom(w, b, n) and N = w+b approaches infinity
	such that p = w/(w + b) remains fixed, then the PMF of X converges to the Bin(n, p) PMF.
	\item \textbf{ Theorem (PMF of g(X))}. Let X be a discrete random variable and $g R->R$. Then the support of g(X) is
	the set of all y such that g(x) = y for at least one x in the support of X, and the PMF of g(X) is
	$P(g(X) = Y) = \sum_{x:g(x)=y} P(X=x)$
	\item \textbf{Definition (Independence of two random variables).} Random variables X and Y are said to be independent
	if
	$P(X \le x, Y \le y) = P(X \le x)P(Y \le y)$
	for all $x, y \in R$. In the discrete case, this is equivalent to the condition
	P(X = x, Y = y) = P(X = x)P(Y = y);
	for all x in the support of X and all y in the support of Y.
\end{enumerate}
\item chapter 3: Expectation
\begin{enumerate}
	\item\textbf{ Proposition (Monotonicity of expectation).} Let X and Y be random variables such that X $\ge$ Y with probability 1. Then E(X)$\ge$E(Y), with equality holding if and only if X = Y with probability 1. 
	\item A hypergeometric can be considered a sum of bernoulli random variables, but with their probabilities conditioned on the previous iteration. However, when calculating hte expectation, each of the bernoullis are equally like to be picked first, so the expecation is n * p where p = w/w+b
	\item \textbf{Theorem. Properties of Indicator Random Variables}\\
	1. $I_A^k=I_A$\\
	2. $I_{A^c} = 1 - I_A$\\
	3. $I_{A\cap B} = I_AI_B$\\
	4. $I_{A\Cup B} - I_A + I_B - I_{A\cap B}$
	\item Inclusion Exclusion \\
	$P(A_1\cup ... A_n) = \sum_iP(A_i)-\sum_{j>i}P(A_i\cap A_j) + ... (-1)^nP(A_1...A_n)$
	\item Negative Binomial: Number of failures till a fixed number of successes. $=> NBin(r,p)P(X=x) = \binom{x+r-1}{r-1}p^rq^{x}$
	\item \textcolor{red}{\textbf{Poisson}} Note $\sum_{k=1}^{\infty}\frac{x^{k-1}}{(k-1)!} = e^x$ Useful for determining expectations and such. its a kernel
	\item \textbf{Theorem. Sum of independent poisson is poisson} if $X\sim Pois(\lambda_1) \, \, \, \& \, \, \, Y \sim Pois(\lambda_2) \, \, X+Y \sim Pois(\lambda_1+\lambda_2)$
	\item \textcolor{red}{\textbf{Theorem. Poisson given a sum of Poissons is Binomial}} $X\sim Pois(\lambda_1) \, \, \, \& \, \, \, Y \sim Pois(\lambda_2) \, \, \, PoisX=k|X+Y=n \sim Binom(n,\lambda_1/(\lambda_1+\lambda_2))$
\end{enumerate}
\item Chapter 4: Continuous
\begin{enumerate}
	\item \textbf{Universality of the Uniform}. This is useful as it can simulate a RV with any discrete Distribution from the Unif.
	\\
	ex\\
	$F(x) = \frac{e^x}{1+e^x} \, \, F^{-1}(u) = log(\frac{u}{1-u}) \sim $logistic with $U \sim Unif(0,1)$ 
	\item \textbf{Normal distribution}\\
	Note that the normal is symmetric, meaning tail areas are symmetric, andi n general $\phi(z)=\phi(-z)$
	\item \textbf{exponential distribution} This is the continuous version of the geometric distribution. $f(x) = \lambda e^{-\lambda x} x>0$
	\item \textcolor{red}{\textbf{Generalize exponential  }}$Y = \frac{X}{\lambda} \sim Expo(\lambda)$ with $X \sim Expo(1)$
	\item see notes for further elaboration on this section and bayesian view point of statistics
\end{enumerate}
\item Chapter 5: Moments
\begin{enumerate}
	\item \textbf{ Definition (Median).} We say that c is a median of a random variable X if $P(X \le c) \ge .5 and P(X \ge c) \ge .5$
	\item \textbf{Definition (Mode).} For a discrete random variable X, we say that c is a mode of X if it maximizes the PMF:
	$P(X = c) \ge P(X = x)$ for all x. For a continuous random variable X with PDF f , we say that c is the mode
	if it maximizes the PDF: $f(c) \ge f (x)$ for all x.\\
	Note we can have multiple modes and medians but not means!
	\item \textbf{Theorem. }Let X be a random variable with mean $\mu$, and let m be the median of X.
	\\The value of c that minimizes the mean squared error $E(X - c)^2$ is c = $\mu$.
	\\A value of c that minimizes the mean absolute error $E|X - c|$ is c = m.
	\item\textbf{ Definition (Symmetry of a random variable). }We say that a random variable X has a symmetric distribution
	about $\mu$ if $X-\mu$ has the same distribution as $\mu - X$. The number $\mu$ must be the mean E(X) if it exists, and
	must also be a median of the distribution of X.
	\item \textcolor{red}{\textbf{Proposition (Symmetry in terms of the PDF).}} Let X be a continuous random variable with PDF f . Then X
	is symmetric about $\mu$ if and only if $f (x) = f (2\mu - x)$ for all x.
	\item\textbf{ Definition (Kinds of moments)}. Let X be a random variable with mean $\mu$ and variance $\sigma^2$. For any positive
	integer n, the nth moment of X is $E(X^n)$, the nth central moment is $E[(X-\mu)^n]$, and the nth standardized
	moment is$E((\frac{X-\mu}{\sigma^2})^n)$
	The mean is the first moment and the variance is the second central moment
	\item \textcolor{red}{\textbf{ Definition (Skewness).}} The skewness of a random variable X with mean $\mu$ and variance $\sigma^2$ is the third
	standardized moment of X \\
	Skew(X) = $E[(\frac{X-\mu}{\sigma^2})^3]$
	\item \textcolor{red}{\textbf{Proposition (Odd central moments of a symmetric distribution). }}Let X be symmetric about its mean $\mu$. Then
	for any odd number m, the mth central moment $E(X -\mu)^m$ is 0 if it exists.
	\item \textbf{kurtosis} This represents how diffent a distributiobns pdf is , large kurtosis means sharp peak in the middle, heavy tails, and low shoulders.
	\item \textbf{Definition (Sample moments)}. Let X1, X2, ... Xn be i.i.d. random variables. The kth sample moment is the random variable\\
	$M_k = \frac{1}{n}\sum_{j=1}^{n}X^k$\\ and
	$E[M_k] = E[\frac{1}{n}\sum_{j=1}^{n}X^k] = E[X_1^k]$ Hence it is an unbiased estimator of the true kth moment
	\item $Var(\bar{X_n}) = \frac{\sigma^2}{n}$
	\item \textbf{Sample variance} = $S_n^2=\frac{1}{n-1}\sum_{j=1}^{n}(X_j-\bar{X_n})^2$
	\item \textcolor{red}{\textbf{Example: Bernoulli MGF}}
	For $X \sim Bern(p)$, the MGF is
	$M(t) = E(e^{tX}) = e^{t0}P(X = 0) + e^{t}P(X = 1) = pet + q$
	where q = 1 - p. Since M(t) is finite for any t $\in$ R, the MGF of a Bernoulli random variable is defined on the entire real line.
	\item \textcolor{red}{\textbf{Example: Geometric MGF}}
	For $X \sim Geom(p)$, the MGF is
	$M(t) = E(e^{tX}) =
	\sum_{k=0}^{\infty}
	e^{tk}P(X = k) = \sum_{k=0}^{\infty}e^{tk}q^kp = \frac{p}{1-qe^t}$
	for $q^{et} < 1$ or, equivalently, for $(-\infty, log(1/q))$,
	\item \textbf{Theorem (Moments via derivatives of MGFs).} Given the MGF M(t) of a random variable X, we can get
	the nth moment of X by evaluating its nth derivative of the MGF at 0
	$E(X^n) = M^{(n')}(0)$
	\item \textcolor{red}{\textbf{Theorem (MGF determines the distribution). }}The MGF of a random variable determines its distribution: if
	two random variables have the same MGF, they must have the same distribution.
	\item \textcolor{red}{\textbf{Theorem (MGF of a sum of independent random variables).}} If X and Y are independent, then the MGF of
	X + Y is the product of the individual MGFs:
	$M_{X+Y} (t) = M_X(t)M_Y(t)$\\
	Using this result, we can obtain the MGFs of the Binomial and Negative Binomial, which are sums of
	independent Bernoulli and Geometric i.i.d. random variables.\\
	\textbf{Example: Binomial MGF}
	The MGF of a Bern(p) random variable is$ pe^t + q$, so the MGF of a Bin(n, p) random variable is
	$M(t) = (pe^t + q)^n$
\end{enumerate}
\item Chapter 6 : Joint Distributions
\begin{enumerate}
	\item
\end{enumerate}
\end{enumerate}
\text{Happy holidays!}
\end{document}
