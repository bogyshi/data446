\documentclass[11pt]{article}
\usepackage{amsgen,amsmath,amstext,amsbsy,amsopn,amssymb}
%\usepackage[dvips]{graphicx,color}
\usepackage{graphicx,color}
\usepackage{graphicx,color,bm}
\usepackage{epsfig}
\usepackage{enumerate}
\usepackage{float}

%\setlength{\oddsidemargin}{0.1 in} \setlength{\evensidemargin}{-0.1
%in} \setlength{\topmargin}{-0.6 in} \setlength{\textwidth}{6.5 in}
%\setlength{\textheight}{8.5 in} \setlength{\headsep}{0.75 in}
%\setlength{\parindent}{0 in} \setlength{\parskip}{0.1 in}

\textwidth 6.3in \textheight 8.8in \topmargin -0.5truein
\oddsidemargin .15truein
\parskip .1in
\renewcommand{\baselinestretch}{1.53}  % double spaced


\newcommand{\homework}[9]{
	\pagestyle{myheadings}
	\thispagestyle{plain}
	\newpage
	\setcounter{page}{1}
	\noindent
	\begin{center}
		\framebox{
			\vbox{\vspace{2mm}
				\hbox to 6.28in { {\bf Math531:~Regression - I  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\Large \hfill #1 (#2)  \hfill} }
				\vspace{6mm}
				\hbox to 6.28in { {\it Instructor: #3 \hfill} }
				\hbox to 6.28in { {\it Office hours: #4  \hfill #6}}
				\vspace{2mm}}
		}
	\end{center}
	\markboth{#1}{#1}
	\vspace*{4mm}
}

% ----------------------- MATH -------------------------
\def\av{\boldsymbol a}
\def\bv{\boldsymbol b}
\def\cv{\boldsymbol c}
\def\dv{\boldsymbol d}
\def\ev{\boldsymbol e}
\def\fv{\boldsymbol f}
\def\gv{\boldsymbol g}
\def\hv{\boldsymbol h}
\def\iv{\boldsymbol i}
\def\gv{\boldsymbol j}
\def\kv{\boldsymbol k}
\def\lv{\boldsymbol l}
\def\mv{\boldsymbol m}
\def\nv{\boldsymbol n}
\def\ov{\boldsymbol o}
\def\pv{\boldsymbol p}
\def\qv{\boldsymbol q}
\def\rv{\boldsymbol r}
\def\sv{\boldsymbol s}
\def\tv{\boldsymbol t}
\def\uv{\boldsymbol u}
\def\vv{\boldsymbol v}
\def\wv{\boldsymbol w}
\def\xv{\boldsymbol x}
\def\yv{\boldsymbol y}
\def\zv{\boldsymbol z}
\def\Av{\boldsymbol A}
\def\Bv{\boldsymbol B}
\def\Cv{\boldsymbol C}
\def\Dv{\boldsymbol D}
\def\Ev{\boldsymbol E}
\def\Fv{\boldsymbol F}
\def\Gv{\boldsymbol G}
\def\Hv{\boldsymbol H}
\def\Iv{\boldsymbol I}
\def\Gv{\boldsymbol J}
\def\Kv{\boldsymbol K}
\def\Lv{\boldsymbol L}
\def\Mv{\boldsymbol M}
\def\Nv{\boldsymbol N}
\def\Ov{\boldsymbol O}
\def\Pv{\boldsymbol P}
\def\Qv{\boldsymbol Q}
\def\Rv{\boldsymbol R}
\def\Sv{\boldsymbol S}
\def\Tv{\boldsymbol T}
\def\Uv{\boldsymbol U}
\def\Vv{\boldsymbol V}
\def\Wv{\boldsymbol W}
\def\Xv{\boldsymbol X}
\def\Yv{\boldsymbol Y}
\def\Zv{\boldsymbol Z}
\def\Abf{\mathbf A}
\def\Bbf{\mathbf B}
\def\Cbf{\mathbf C}
\def\Dbf{\mathbf D}
\def\Ebf{\mathbf E}
\def\Fbf{\mathbf F}
\def\Gbf{\mathbf G}
\def\Hbf{\mathbf H}
\def\Ibf{\mathbf I}
\def\Gbf{\mathbf J}
\def\Kbf{\mathbf K}
\def\Lbf{\mathbf L}
\def\Mbf{\mathbf M}
\def\Nbf{\mathbf N}
\def\Obf{\mathbf O}
\def\Pbf{\mathbf P}
\def\Qbf{\mathbf Q}
\def\Rbf{\mathbf R}
\def\Sbf{\mathbf S}
\def\Tbf{\mathbf T}
\def\Ubf{\mathbf U}
\def\Vbf{\mathbf V}
\def\Wbf{\mathbf W}
\def\Xbf{\mathbf X}
\def\Ybf{\mathbf Y}
\def\Jbf{\mathbf J}
\def\Zbf{\mathbf Z}
\def\Am{\mathrm A}
\def\Bm{\mathrm B}
\def\Cm{\mathrm C}
\def\Dm{\mathrm D}
\def\Em{\mathrm E}
\def\Fm{\mathrm F}
\def\Gm{\mathrm G}
\def\Hm{\mathrm H}
\def\Im{\mathrm I}
\def\Gm{\mathrm J}
\def\Km{\mathrm K}
\def\Lm{\mathrm L}
\def\Mm{\mathrm M}
\def\Nm{\mathrm N}
\def\Om{\mathrm O}
\def\Pm{\mathrm P}
\def\Qm{\mathrm Q}
\def\Rm{\mathrm R}
\def\Sm{\mathrm S}
\def\Tm{\mathrm T}
\def\Um{\mathrm U}
\def\mv{\mathrm V}
\def\Wm{\mathrm W}
\def\Xm{\mathrm X}
\def\Ym{\mathrm Y}
\def\Zm{\mathrm Z}
\newcommand{\Ac}{\mathcal{A}}
\newcommand{\Bc}{\mathcal{B}}
\newcommand{\Cc}{\mathcal{C}}
\newcommand{\Dc}{\mathcal{D}}
\newcommand{\Ec}{\mathcal{E}}
\newcommand{\Fc}{\mathcal{F}}
\newcommand{\Gc}{\mathcal{G}}
\newcommand{\Hc}{\mathcal{H}}
\newcommand{\Ic}{\mathcal{I}}
\newcommand{\Jc}{\mathcal{J}}
\newcommand{\Kc}{\mathcal{K}}
\newcommand{\Lc}{\mathcal{L}}
\newcommand{\Mc}{\mathcal{M}}
\newcommand{\Nc}{\mathcal{N}}
\newcommand{\Oc}{\mathcal{O}}
\newcommand{\Pc}{\mathcal{P}}
\newcommand{\Qc}{\mathcal{Q}}
\newcommand{\Rc}{\mathcal{R}}
\newcommand{\Sc}{\mathcal{S}}
\newcommand{\Tc}{\mathcal{T}}
\newcommand{\Uc}{\mathcal{U}}
\newcommand{\Vc}{\mathcal{V}}
\newcommand{\Wc}{\mathcal{W}}
\newcommand{\Xc}{\mathcal{X}}
\newcommand{\Yc}{\mathcal{Y}}
\newcommand{\Zc}{\mathcal{Z}}
\newcommand{\alphav}{\mbox{\boldmath{$\alpha$}}}
\newcommand{\betav}{\mbox{\boldmath{$\beta$}}}
\newcommand{\gammav}{\mbox{\boldmath{$\gamma$}}}
\newcommand{\deltav}{\mbox{\boldmath{$\delta$}}}
\newcommand{\epsilonv}{\mbox{\boldmath{$\epsilon$}}}
\newcommand{\zetav}{\mbox{\boldmath$\zeta$}}
\newcommand{\etav}{\mbox{\boldmath{$\eta$}}}
\newcommand{\iotav}{\mbox{\boldmath{$\iota$}}}
\newcommand{\kappav}{\mbox{\boldmath{$\kappa$}}}
\newcommand{\lambdav}{\mbox{\boldmath{$\lambda$}}}
\newcommand{\muv}{\mbox{\boldmath{$\mu$}}}
\newcommand{\nuv}{\mbox{\boldmath{$\nu$}}}
\newcommand{\xiv}{\mbox{\boldmath{$\xi$}}}
\newcommand{\omicronv}{\mbox{\boldmath{$\omicron$}}}
\newcommand{\piv}{\mbox{\boldmath{$\pi$}}}
\newcommand{\rhov}{\mbox{\boldmath{$\rho$}}}
\newcommand{\sigmav}{\mbox{\boldmath{$\sigma$}}}
\newcommand{\tauv}{\mbox{\boldmath{$\tau$}}}
\newcommand{\upsilonv}{\mbox{\boldmath{$\upsilon$}}}
\newcommand{\phiv}{\mbox{\boldmath{$\phi$}}}
\newcommand{\varphiv}{\mbox{\boldmath{$\varphi$}}}
\newcommand{\chiv}{\mbox{\boldmath{$\chi$}}}
\newcommand{\psiv}{\mbox{\boldmath{$\psi$}}}
\newcommand{\omegav}{\mbox{\boldmath{$\omega$}}}
\newcommand{\Sigmav}{\mbox{\boldmath{$\Sigma$}}}
\newcommand{\Lambdav}{\mbox{\boldmath{$\Lambda$}}}
\newcommand{\Deltav}{\mbox{\boldmath{$\Delta$}}}
\newcommand{\Omegav}{\mbox{\boldmath{$\Omega$}}}
\newcommand{\varepsilonv}{\mbox{\boldmath{$\varepsilon$}}}

\newcommand{\eps}{\varepsilon}
\newcommand{\epsv}{\mbox{\boldmath{$\varepsilon$}}}

\def\1v{\mathbf 1}
\def\0v{\mathbf 0}
\def\Id{\mathbf I} % identity matrix
\newcommand{\ind}[1]{\mathbbm{1}_{\left[ {#1} \right] }}
\newcommand{\Ind}[1]{\mathbbm{1}_{\left\{ {#1} \right\} }}
\newcommand\indep{\protect\mathpalette{\protect\independenT}{\perp}}\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\QED}{\begin{flushright} {\bf QED} \end{flushright}}
\newcommand{\R}{\mathbb R}
\newcommand{\Real}{\mathbb R}
\newcommand{\C}{\mathbb C}
\newcommand{\E}{\mathbb E}
\newcommand{\sgn}{\mathop{\mathrm{sign}}}
\def\Pr{\mathrm P}
\def\pr{\mathrm P}
\newcommand{\Var}{\mathop{\rm Var}}
\newcommand{\var}{\mathop{\rm Var}}
\newcommand{\Cov}{\mathop{\rm Cov}}
\newcommand{\cov}{\mathop{\rm Cov}}
\newcommand{\Corr}{\mathop{\rm Corr}}
\newcommand{\ang}{\mathop{\rm Angle}}
\newcommand{\tr}{\mathop{\rm trace}}
\newcommand{\proj}{\mathop{\rm Proj}}
\newcommand{\rank}{\mathop{\rm rank}}

\newcommand{\diag}{\mathop{\rm diag}}
\newcommand{\Diag}{\mathop{\rm diag}}
\newcommand{\sk}{\vspace{0.5cm}}
\newcommand{\ds}{\displaystyle}
\newcommand{\mb}{\mbox}
\newcommand{\wh}{\widehat}
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
\newcommand{\argmax}{\operatornamewithlimits{argmax}}

\newcommand{\norm}[1]{\|#1\|}
\newcommand{\abs}[1]{\left\vert#1\right\vert}
\newcommand{\set}[1]{\left\{#1\right\}}

\newcommand{\To}{\longrightarrow}

\def\equalLaw{\stackrel{\mathcal{L}}{=}}
\def\equallaw{\stackrel{\mathcal{L}}{=}}

\def\half{\frac{1}{2}}

\usepackage{caption}

\begin{document}

\begin{title}
	{\Large\bf Homework 3, DATA 556: Due Tuesday, 10/16/2018}
\end{title}

\author{\bf Alexander Van Roijen}

\maketitle

\newpage
Please complete the following:
\begin{enumerate}
\item Problem 1
\begin{enumerate}
	\item Find the mean and the variance of a Discrete Uniform random variable on 1, 2, . . . , n.\\
	Let $X \sim $ DUnif
	\begin{gather}
	P(X = x) = \frac{1}{n} \text{ by the def of uniform distrib}\\
	E[X] = \sum_{j=0}^{n} P(X=j)*j = \sum_{j=0}^{n} \frac{1}{n}*j = \frac{1}{n} *\sum_{j=0}^{n} j = \frac{1}{n} * \frac{n(n+1)}{2} = \frac{n+1}{2}\\
	\text{It is a similar proof with a different subsitution for the expected value of X squared} \\
	E[X^2] = \sum_{j=0}^{n} P(X=j)*(j^2) \text{ LOTUS} \\
	= \sum_{j=0}^{n} \frac{1}{n}*(j^2) = \frac{1}{n} *\sum_{j=0}^{n} (j^2) = \frac{1}{n} * \frac{n(n+1)(2n+1)}{6} = \frac{(n+1)(2n+1)}{6}\\
	E[X^2]-(E[X])^2 = \frac{(n+1)(2n+1)}{6} - (\frac{n+1}{2})^2\\
	\frac{4*(n+1)(2n+1)}{24} - (\frac{6*(n+1)^2}{24}) = \frac{n^2-1}{12}
	\end{gather}
	\item Let X be a discrete random variable that satisfies the properties listed in the homework. Find E[X]
	\begin{gather}
		\text{We have a symmetry such that } P(X=x)=P(X=-x)\\
		E[X]= \sum_{x=-n}^{n} P(X=x)*x\\
		= P(X=-n)*-n + P(X=-(n-1))*(-(n-1)) + ... + P(X=n)*n \\
		= -1*(P(X=-n)*n - P(X=n)*n) + ... P(X=0)*0 \\
		=  -1*(P(X=n)*n - P(X=n)*n) + ... P(X=0)*0 = 0+0+...+0 \\
		\text{due to the symmetry of the probs}\\
		=> E[X] = 0
	\end{gather}
\end{enumerate}
\item We have X with PMF $P(X = k) = \frac{-1*p^k}{log(1-p)*k}$
for k = 1, 2, . . .. Here p is a parameter with 0 < p < 1. Find the mean and the variance of X
\\
First, note that the value $\frac{-1}{log(1-p)}$ is a positive constant
\begin{gather}
	E[X] = \sum_{k=1}^{\infty} P(X=k)*k = \sum_{k=1}^{\infty} \frac{-1*p^k}{log(1-p)*k} * k = \sum_{x=1}^{\infty} \frac{-1*p^k}{log(1-p)} \\ 
	= \frac{-1}{log(1-p)} \sum_{x=1}^{\infty} p^k \\
	=> \sum_{x=1}^{\infty} p^k = S => S*p = \sum_{x=2}^{\infty} p^k => S-Sp = p => S= \frac{p}{(1-p)}\\
	=> E[X] = \frac{-1}{log(1-p)} * \frac{p}{(1-p)}
	\text{similarly, the }E[X^2] \text{is proved in a similar manner except} \\
	S-Sp = \sum_{x=1}^{\infty} p^k \\
	\text{which we already proved so by substituting we get}\\
	S-Sp =  \frac{p}{(1-p)} => S = \frac{p}{(1-p)^2}\\
	=> E[X^2] = \frac{-1}{log(1-p)} * \frac{p}{(1-p)^2}\\
	=>Var(X) = E[X^2]-E[X]^2 = \frac{p(p-log(1-p))}{log(1-p)^2*(1-p)^2}
\end{gather}	
\item
\begin{enumerate}
	\item  Use LOTUS to show that for X $\sim$ Pois($\lambda$) and any function g(.), $E[Xg(X)] = \lambda E[g(X+1)]$
	\begin{gather}
		E[Xg(X)] = \sum_{x=0}^{\infty} x*g(x) \frac{e^{-\lambda} * \lambda ^ x}{x!} = \sum_{x=1}^{\infty} g(x)*\lambda \frac{e^{-\lambda} * \lambda ^{x-1}}{(x-1)!} \\
		\text{now let there be a substitution such that j = x-1}\\
		= \sum_{j=0}^{\infty} g(j+1)*\lambda \frac{e^{-\lambda} * \lambda ^{j}}{j!} =  \lambda * \sum_{j=0}^{\infty} g(j+1) \frac{e^{-\lambda} * \lambda ^{j}}{j!} \\
		\text{considering g(x) is any valid function, we conclude}\\
		E[Xg(X)] = \lambda * E[g(X+1)] \\
		\text{We bring this back to this form using LOTUS and our lack of knowledge on g(x)}
	\end{gather}
	\item Find the third moment E($X^3$) for X âˆ¼ Pois($\lambda$)
	\begin{gather}
		\text{let }g(x) = x^2
		=> E(Xg(X))=E[g(X+1)]=\lambda * E[(X+1)^2]\\
		= \lambda * E[X^2 + 2X+1] = \lambda * (E[X^2] + 2E[X] + 1) \\
		\text{using the known properties of the variance and mean of the uniform dist we get}\\
		= \lambda * (\lambda^2 + \lambda + 2\lambda + 1) = \lambda^3 + 3\lambda^2 + \lambda
	\end{gather}
\end{enumerate}
\item Show that for any events $A_1..A_n$,
$P(A_1\cap...\cap A_n) \geq \sum_{j=1}^{n} P(A_j) - n +1$\\
lets first begin by studying this same situation for indicator random variables for these events.\\
One more thing to keep in mind is that we know\\
$0\leq P(X=x)\leq1 \forall x$ and similarly $I(A)=1$ or $I(A)=0$
first we show 	$I(A_1\cap...\cap A_n) \geq \sum_{j=1}^{n} I(A_j) - n +1$
\begin{gather} 
	\text{Case 1:} I(A_1\cap...\cap A_n) = 0\\
	=> \sum_{j=1}^{n} I(A_j) \leq (n-1)\\ 
	\text{this is self explanatory as at most n-1 indicator terms are 1}\\
	=> \sum_{j=1}^{n} I(A_j) - n + 1 \leq I(A_1\cap...\cap A_n)\\
	\text{Case 2:} I(A_1\cap...\cap A_n) = 1\\
	=> \sum_{j=1}^{n} I(A_j) = n\\ 
	\text{this is self explanatory as all n indicator variables must be 1}\\
	=> \sum_{j=1}^{n} I(A_j) - n + 1 = n-n+1 = 1 \leq I(A_1\cap...\cap A_n) = 1\\
	\text{Now we look at the broader case of } P(A_1\cap...\cap A_n) \geq \sum_{j=1}^{n} P(A_j) - n +1\\
	\text{we know that the total law of probaility follows that }\\
	\text{can we say sum of all A probs must be less or equal to 1?}\\
	\text{or can we say that the expected value of inidcator = P implies its true?}
\end{gather}

\item A copy machine is used to make n pages of copies per day. The machine has two trays in which paper gets
loaded, and each page is taken randomly and independently from one of the other trays. At the beginning
of the day, the trays are refilled so that they each have m pages. Using simulations in R, find the smallest
value of m for which there is at least a 95 percent chance that both trays have enough paper on a particular day,
for n = 10, n = 100, n = 1000, and n = 10000.
\begin{verbatim}

	n=c(10,100,1000,10000)

	findMinM2 = function(n,p){
		m=(n/2) 
		numTries=10000
		while (TRUE) {
			numCorrect=0
			results = rbinom(numTries,n,0.5)
			l = ((m-results)>=0)
			r = ((m - (n-results))>=0)
			numCorrectvec=l&r
			numCorrect=length(numCorrectvec[numCorrectvec=="TRUE"])
			#print(numCorrect)
			sum = numCorrect/numTries
			if(sum>=0.95)
			{
				return(m)
			}
			else
			{
				m= m+1
			}
	}
	
	for (ns in n)
	{
		print(findMinM2(ns,0.5))
	}
	> [1] 8
	> [1] 60
	> [1] 531
	> [1] 5097
\end{verbatim}

\text{it is worth noting that this is easily checked using pbinom. This code can be found on my github!}
\end{enumerate}

\end{document}
